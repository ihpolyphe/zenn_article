---
title: "WSL2でmusikaでfinetuning"
emoji: "📌"
type: "tech"
topics:
  - "tensorflow"
  - "wsl2"
  - "musika"
  - "音楽生成ai"
published: true
published_at: "2023-05-20 10:25"
---

# はじめに
前回は音楽生成AIの[magenta](https://github.com/magenta/magenta)のMusic Transfomerを使用してmidiファイルを入力として続きの音楽を生成してみました。その際の記事は以下です。
https://zenn.dev/articles/fa8f56e67fc6e7/edit

ただ、生成した曲はあまり納得のいくものではなかったので、今回は`musika`を使用して好きな曲をファインチューニングで学習させて、自分好みの音楽が生成できるかを試してみようと思います。

## musikaとは
2022年の8月に採択された音楽生成AI。GANベースで個人PCでも高速に学習ができるとのこと。
オンラインデモや、colabも用意されているので、気軽に試すことができます。
- 公式GitHub：https://github.com/marcoppasini/musika/tree/main
- 論文：https://arxiv.org/abs/2208.08706
- Onlineデモ：https://huggingface.co/spaces/marcop/musika
- Finetuning公式Colab notebook：https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb

# 実行環境
環境
WSL2がセットアップされていることが前提です。
ホストOS: Windows11
WSL2: 5.10.16.3-microsoft-standard-WSL2(Ubuntu 20.04)
CPU: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz 2.30 GHz
GPU:NVIDIA GeForce RTX 3050 Ti Laptop GPU

WSL2とAnacondaがインストールされていることが前提です。以下を参考にセットアップさせていただきました。
- https://qiita.com/burugaria7/items/4005724c5d1b5228327e

また、今回GPUを使用して学習させています。CPUで実行すると250epochの1epochの1%に5分くらいかかりました。単純計算、完了時間=5分*100*250=2000時間くらいかかりそうだったのでGPUを使用しています。

WSL2でGPUを使用する方法は以下を参考にさせていただきました。
https://zenn.dev/rhene/articles/af7a0162ee3a6332decc

# musikaのセットアップ
公式GitHubに丁寧にセットアップ方法が書いてあるので、特に詰まったところはありませんでした。
condaでpython3.9環境を作成し、
```
conda create -n musika python=3.9
```
作成出来たらactivateします。
```
conda activate musika
```
https://zenn.dev/rhene/articles/af7a0162ee3a6332deccを参考に、cuda11.2をインストールしているので、以下のコマンドでCUDAをインストールします。
```
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
```
WSL2なので以下も実行しました。
```
mkdir -p $CONDA_PREFIX/etc/conda/activate.d

echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```
最後に、`musika`をクローンして、必要なライブラリをインストールしたらセットアップ完了です。
```
git clone https://github.com/marcoppasini/musika
cd musika
pip install -r requirements.txt
```

## musika実行
`musika`が動かせるかを確認します。
以下のコマンドでローカルブラウザで音楽サンプルが生成できるかを確認できます。
```
python musika_test.py --load_path checkpoints/misc
```
こちらのコマンドで10個のサンプルミュージックを生成できます。
```
python musika_generate.py --load_path checkpoints/misc --num_samples 10 --seconds 120 --save_path generations
```

# Finetuning
最後にファインチューニングの実行を行います。まずは学習させたい音楽を用意します。mp3や、wavファイルなどは取り込めました。midiとかもいけるのかもしれないですね。

まずは学習させたい音楽フォルダを指してAutoEncodingさせます。以下のコマンドです。
```
python musika_encode.py --files_path folder_of_audio_files --save_path folder_of_encodings --whole True
```
こちらを実行するとnpyファイルが生成されるので、これを入力として学習させるという流れです。

あとは以下のコマンドでFinetuningが開始されます。1%10秒もかかってないので、250epoch70時間、、、てめちゃめちゃ時間かかるやん！ということでepoch数は調整して実行しましょう。
```
python musika_train.py --train_path folder_of_encodings --load_path checkpoints/misc --lr 0.00004 --epochs 20
```
以下でepoch数が`epochs`の引数で設定できそうですね。
https://github.com/marcoppasini/musika/blob/main/parse/parse_train.py#L118

# 終わりに
今回は`musika`のセットアップ方法と、Finetuningを行うまでをまとめました。次は学習させたモデルを使って曲を生成させて聞いてみようと思います。最後まで読んだいただきありがとうございました。