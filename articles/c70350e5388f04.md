---
title: "Stable Audio Openでプロンプトから音楽生成を試してみた"
emoji: "🗂"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Stable Audio Open", "Prompt Programing", "Music"]
published: true
published_at: "2024-6-13"
---
# はじめに
`stable diffusion`が一世を風靡していましたが、その音楽版モデルがリリースされたということで使ってみました。

簡単に使うところまではできたのでひとまず本記事にまとめます。ただ、`stable diffusion`でもそうでしたが、本番はプロンプトを使用して期待通りの出力（今回は音楽）を得ることだと思うので、それについてはいろいろ試してみてまた記事にまとめられればと思います。

[stable-audio-open-1.0](https://huggingface.co/stabilityai/stable-audio-open-1.0)

# 実行環境
環境
WSL2がセットアップされていることが前提です。今回はほかの環境を汚したくないので`venv`を使用してpipで環境構築しました。(condaを使用したり環境ぐちゃぐちゃなのでそのうち整理したい。。。)

ホストOS: Windows11
WSL2: 5.10.16.3-microsoft-standard-WSL2(Ubuntu 20.04)
CPU: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz 2.30 GHz
GPU:NVIDIA GeForce RTX 3050 Ti Laptop GPU

## セットアップ
### 仮想環境構築
まずは仮想環境(`venv`)の構築を行います。["Pythonの環境構築をマスターする(pyenv,venv)(WSL2,Ubuntu利用)(poetry追記)"](https://zenn.dev/tigrebiz/articles/2822fb4de256d8)を参考にさせていただきました。

適当なディレクトリを作成し、移動して新しい仮想環境を作成します。
```
mkdir stable-audio-open
cd stable-audio-open
python -m venv stable-audio-open
```

仮想環境の作成に成功したら以下のコマンドで仮想環境をactivateします。無事仮想環境をactivateできたらpipを一応upgradeしておきました。
```
source stable-audio-open/bin/activate
pip install --upgrade pip
```

### stable audio toolsインストール
`stable audio open`は[`stable-audio-tools`](https://github.com/Stability-AI/stable-audio-tools)に含まれるので公式README.mdに従ってインストールを行いましょう。

以下でインストールしていますが、inferenceするためには公式リポジトリのクローンインストールする必要があるので、こちらはいらないのかも？
```
pip install stable-audio-tools
```
inferenceするためには公式リポジトリのクローンインストールする必要があるので以下は実行しましょう。
```
git clone https://github.com/Stability-AI/stable-audio-tools.git
cd stable-audio-tools
pip install .
```

### Huggingfaceログイン
`stable audio open`は`Huggingface`によってモデルが管理されているので、ログインし、トークンを取得しないとモデルを使用できないです。

https://huggingface.co/stabilityai/stable-audio-open-1.0にアクセスすると`stable audio open`を利用するための登録などがありますので、実施してください。

また、モデルの使用時にトークンが設定されている必要があります（GitHubのPATのようなものですね）。

トークンの取得方法はググれば出てくるのでここでは割愛させていただきます。トークンの設定方法は、自分の方法が正しいとは思ってないですが、以下の方法で設定できました。

普通にpythonから設定した感じですね。

```
(stable-audio-open) (base) user:~/stable-audio-open$ python
Python 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from huggingface_hub import login
>>> login()

    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .
Enter your token (input will not be visible):
Add token as git credential? (Y/n) y
Token is valid (permission: read).

Token has not been saved to git credential helper.
Your token has been saved to /home/user/.cache/huggingface/token
Login successful
```

## pythonスクリプトを作成して曲を生成
ここまでくればあとは曲を生成するだけです。公式がサンプルコードを提供しているのでそのまま使用すれば使えます。あとはお好みにpromtを変えるだけですね。
それではみなさんも楽しいプロンプト生活をお楽しみください。

```python
import torch
import torchaudio
from einops import rearrange
from stable_audio_tools import get_pretrained_model
from stable_audio_tools.inference.generation import generate_diffusion_cond

device = "cuda" if torch.cuda.is_available() else "cpu"

# Download model
model, model_config = get_pretrained_model("stabilityai/stable-audio-open-1.0")
sample_rate = model_config["sample_rate"]
sample_size = model_config["sample_size"]

model = model.to(device)
prompt = "128 BPM tech house drum loop"
# Set up text and timing conditioning
conditioning = [{
    "prompt": prompt,
    "seconds_start": 0, 
    "seconds_total": 47
}]

# Generate stereo audio
output = generate_diffusion_cond(
    model,
    steps=100,
    cfg_scale=7,
    conditioning=conditioning,
    sample_size=sample_size,
    sigma_min=0.3,
    sigma_max=500,
    sampler_type="dpmpp-3m-sde",
    device=device
)

# Rearrange audio batch to a single sequence
output = rearrange(output, "b d n -> d (b n)")

# Peak normalize, clip, convert to int16, and save to file
output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()
torchaudio.save(prompt+".wav", output, sample_rate)
```